{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2oZo0SfityZ",
    "outputId": "26494094-624d-465b-fe94-d7db35094ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      ",\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      ",\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      ",Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      ",Collecting datasets\n",
      ",  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      ",Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      ",Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
      ",Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      ",Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      ",Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      ",Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      ",Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      ",  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      ",Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      ",Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      ",Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      ",Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      ",  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      ",Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      ",Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      ",Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      ",Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      ",Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      ",Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      ",Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      ",Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      ",Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      ",Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      ",Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      ",Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      ",Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      ",Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      ",Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      ",Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      ",Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      ",Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      ",Collecting multiprocess<0.70.17 (from datasets)\n",
      ",  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      ",Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
      ",Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      ",Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      ",Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      ",Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      ",Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      ",Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      ",Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      ",Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      ",Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      ",Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      ",Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      ",Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      ",Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      ",Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      ",Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      ",Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      ",Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      ",Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      ",Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      ",Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      ",Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      ",Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      ",Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      ",\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cudnn-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      ",\u001b[0mInstalling collected packages: nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
      ",  Attempting uninstall: nvidia-cusolver-cu12\n",
      ",    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      ",    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      ",      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      ",Successfully installed datasets-3.5.0 multiprocess-0.70.16 nvidia-cudnn-cu12-9.1.0.70 nvidia-cusolver-cu12-11.6.1.9\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "7967eeab52e44a0abff67b2673434b56",
      "9e0bd52634c84d8ea7ac3ef629531abe",
      "a555053bd3c640209882061e25916eef",
      "48dc565357604d658d7b45b382f4afa3",
      "e5c4d51d2caa4667a6d1424fb49677b2",
      "1ad57637b94544c4b63856b31c6dfc05",
      "45b12e09e946461b8977a07db5c77163",
      "9cb589c751b14cb3b8b678a667b5edc1",
      "784d4b79e37a4775b42b7acfe210f6b3",
      "61afb8ba55574dc281837fcb104f7217",
      "c701c88642404ad89bcae46ae5290439",
      "cfc1f697ee2545fd8cfb71ae281d485b",
      "b2e780586e994c9b8e8e312e86b11f8f",
      "2b397e4ec9ee4131965fdb0aa8a97800",
      "3c1b5802ec2340b5a9561d1adcd2f750",
      "aa0fb5bc510449bda55611516df50efd",
      "c8fa36e3685a49cb834a0e6d24ccc3e7",
      "4443daa62444428eb8604d4f6bec4e9c",
      "2b9cf96953c04471b057d477cd0afc33",
      "7068cbb3f68140a79de954ced4e9bdf6",
      "8f106164c87548178b31c6791a1fb801",
      "6ce288b25e914b7aa730cfcde7a74206",
      "ce7975d1e3e2400eb5b41f33eddd1aab",
      "334ecd470c2e4af8a85d477cef746498",
      "286e3f8cd5724637b02d755066073fd0",
      "dadaf9584b60445aa0eb7e6975b0ef48",
      "ed0ca6e8ddc4486dbc654cfa273af867",
      "039287aed73c421fbd2035463f6e27c8",
      "1d016e208ea445e7a62fd74e8d09e9e5",
      "2006837973e341c7b999cfad7607e242",
      "b506dd7ed434423c8f41ab7a9a52695a",
      "d4b27964bcc14f05a1a19e7122962e4e",
      "fbce0e2db10640189808bcf81a1bfe29",
      "c4c49da29752448e9d8a37fedd427abc",
      "75c59f207d79429e853aba2fa6f2bb37",
      "6ee26c6d86584dadba7028e003854c57",
      "8d7624dd49a046d1bf195fc0ba8ebb2f",
      "64c7228fe9394ce9a4f5b27bc8ad9ad9",
      "a148251c81c143cdaba5cdf34ea48d1d",
      "13b7acbdd7a44bc1ae1355daad3e83e3",
      "1c11d719474e4947aaf5c44a1ba1fe63",
      "61df4fa14d0644f095366ff5a1b397b8",
      "33c57f56da2247d6a357f54d325bd238",
      "1ac6ff9e00e1432cae1e0840ea4f9d47"
     ]
    },
    "id": "vvHFJao2i63u",
    "outputId": "6cfd7f0a-3c24-44a3-ecc0-88a8b7fe30cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      ",The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      ",To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      ",You will be able to reuse this secret in all of your notebooks.\n",
      ",Please note that authentication is recommended but still optional to access public models or datasets.\n",
      ",  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7967eeab52e44a0abff67b2673434b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc1f697ee2545fd8cfb71ae281d485b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7975d1e3e2400eb5b41f33eddd1aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c49da29752448e9d8a37fedd427abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Drop unnecessary index column\n",
    "df = df.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Encode labels: 'on-topic' -> 1, 'off-topic' -> 0\n",
    "df[\"label\"] = df[\"label\"].map({\"on-topic\": 1, \"off-topic\": 0})\n",
    "\n",
    "# Drop NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Train-test split (80% train, 20% validation)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"tweet\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize tweets\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_encodings[\"input_ids\"])\n",
    "train_masks = torch.tensor(train_encodings[\"attention_mask\"])\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "val_inputs = torch.tensor(val_encodings[\"input_ids\"])\n",
    "val_masks = torch.tensor(val_encodings[\"attention_mask\"])\n",
    "val_labels = torch.tensor(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jyiFiYQLjCBL"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, inputs, masks, labels):\n",
    "        self.inputs = inputs\n",
    "        self.masks = masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[idx],\n",
    "            \"attention_mask\": self.masks[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_labels = train_labels.to(torch.long)\n",
    "val_labels = val_labels.to(torch.long)\n",
    "train_dataset = DisasterDataset(train_inputs, train_masks, train_labels)\n",
    "val_dataset = DisasterDataset(val_inputs, val_masks, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908,
     "referenced_widgets": [
      "32d00b4671dd4aca80e99c3ff1b2d3c5",
      "99060eb3fa6244deb00ccd9aa4624b3c",
      "fec18e7e076840e4be5b83c2fc9e05c2",
      "9cf75d2a814a445c9bc5ed7c318e445c",
      "f93c882dcf944ec4bf6e4632a73f9e79",
      "b852243b5204446c9e7e492f09b55082",
      "67be5ab7683c418990ae21fbe569decf",
      "0c0f8d87553e40db85d6c40bbbe943b8",
      "2cfa765820d54a0c904c766e73a6b0cf",
      "99a5483968a0498f820a187dd5af095a",
      "6ecfea2a7954497396365b854e1821fe"
     ]
    },
    "id": "PggISvBEjHIC",
    "outputId": "92e09945-eeb7-46ec-cc79-d3b953790a38"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d00b4671dd4aca80e99c3ff1b2d3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      ",You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load BERT with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLmUFx2VlCKm",
    "outputId": "8e89fa8d-e050-4b40-bf84-50f24df97858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      ",Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      ",Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      ",Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      ",Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      ",Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      ",Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      ",Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      ",Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      ",Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      ",Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      ",Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      ",Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      ",Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      ",Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      ",Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      ",Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NjrJzD4l8CA",
    "outputId": "e605f865-dc4c-439c-ca09-f6a41c6747f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      ",Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      ",Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      ",Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      ",Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
      ",Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      ",Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      ",Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      ",Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      ",Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      ",Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      ",Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      ",Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      ",Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      ",Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      ",Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      ",Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      ",Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      ",Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      ",Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      ",Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      ",Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      ",Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      ",Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      ",Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      ",Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      ",Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      ",Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      ",Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      ",Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      ",Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      ",Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      ",Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      ",Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      ",Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
      ",Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      ",Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      ",Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      ",Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      ",Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      ",Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      ",Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      ",Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      ",Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      ",Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      ",Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      ",Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      ",Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      ",Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      ",Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      ",Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      ",Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      ",Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      ",Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a6a6tqcWjL0P"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udFJ31lBjN99",
    "outputId": "0edbf87a-3bc7-43e9-e31d-35b1d5262c21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 366/366 [01:06<00:00,  5.48it/s, loss=0.0278]\n",
      ",Epoch 2: 100%|██████████| 366/366 [01:06<00:00,  5.50it/s, loss=0.00247]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define learning rate scheduler\n",
    "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Training loop\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjS5KF2ZjPlr",
    "outputId": "32fc589e-df64-470d-fec8-eae949d5f9a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_disaster_model/tokenizer_config.json',\n",
       " 'bert_disaster_model/special_tokens_map.json',\n",
       " 'bert_disaster_model/vocab.txt',\n",
       " 'bert_disaster_model/added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"bert_disaster_model\")\n",
    "tokenizer.save_pretrained(\"bert_disaster_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VB1ykVlEoU__",
    "outputId": "360d5ff3-021c-4dbd-82bb-743c89181b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FW0diA8B5EaR",
    "outputId": "f01e7e8d-9104-4969-d9ca-b83824653464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ",Testing model on example tweets:\n",
      ",\n",
      ",Tweet: BREAKING NEWS: Earthquake magnitude 7.1 hits coastal region, tsunami warning issued #emergency\n",
      ",Prediction: on-topic\n",
      ",Confidence: 0.9578\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Our thoughts are with everyone affected by the floods, stay safe!\n",
      ",Prediction: on-topic\n",
      ",Confidence: 0.9977\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Just watched the latest Marvel movie and it was amazing!\n",
      ",Prediction: off-topic\n",
      ",Confidence: 0.9993\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Today's weather forecast shows clear skies and warm temperatures\n",
      ",Prediction: off-topic\n",
      ",Confidence: 0.9582\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Volunteers needed to help with hurricane relief efforts, please RT\n",
      ",Prediction: on-topic\n",
      ",Confidence: 0.9950\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Fire spreading through downtown area, evacuation orders in place\n",
      ",Prediction: on-topic\n",
      ",Confidence: 0.9982\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: The new restaurant on Main Street has delicious food\n",
      ",Prediction: off-topic\n",
      ",Confidence: 0.9994\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Happy birthday to my best friend! Love you lots!\n",
      ",Prediction: off-topic\n",
      ",Confidence: 0.9994\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: URGENT: Missing child in Springfield area, please share description\n",
      ",Prediction: off-topic\n",
      ",Confidence: 0.9993\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Traffic is terrible this morning, expect delays on Highway 101\n",
      ",Prediction: off-topic\n",
      ",Confidence: 0.9869\n",
      ",--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Function to make predictions on new tweets\n",
    "def predict_disaster(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoded_text = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "    input_ids = encoded_text['input_ids'].to(device)\n",
    "    attention_mask = encoded_text['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        _, predictions = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "    return {\n",
    "        'prediction': predictions.item(),\n",
    "        'on_topic_probability': probs[0][1].item(),\n",
    "        'off_topic_probability': probs[0][0].item(),\n",
    "        'label': 'on-topic' if predictions.item() == 1 else 'off-topic'\n",
    "    }\n",
    "\n",
    "# Make sure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example tweets to test\n",
    "test_tweets = [\n",
    "    \"BREAKING NEWS: Earthquake magnitude 7.1 hits coastal region, tsunami warning issued #emergency\",\n",
    "    \"Our thoughts are with everyone affected by the floods, stay safe!\",\n",
    "    \"Just watched the latest Marvel movie and it was amazing!\",\n",
    "    \"Today's weather forecast shows clear skies and warm temperatures\",\n",
    "    \"Volunteers needed to help with hurricane relief efforts, please RT\",\n",
    "    \"Fire spreading through downtown area, evacuation orders in place\",\n",
    "    \"The new restaurant on Main Street has delicious food\",\n",
    "    \"Happy birthday to my best friend! Love you lots!\",\n",
    "    \"URGENT: Missing child in Springfield area, please share description\",\n",
    "    \"Traffic is terrible this morning, expect delays on Highway 101\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting model on example tweets:\\n\")\n",
    "for tweet in test_tweets:\n",
    "    result = predict_disaster(tweet, model, tokenizer, device)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Prediction: {result['label']}\")\n",
    "    print(f\"Confidence: {max(result['on_topic_probability'], result['off_topic_probability']):.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsc5r_1K7oJb",
    "outputId": "346652d5-05dd-4fc4-fa55-7f689cd950aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Stage Disaster Classification Results:\n",
      ",\n",
      ",Tweet: BREAKING NEWS: Earthquake magnitude 7.1 hits coastal region, tsunami warning issued #emergency\n",
      ",Classification: ON-TOPIC - EARTHQUAKE\n",
      ",Confidence (on-topic): 0.9578\n",
      ",Confidence (disaster type): 0.9500\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Explosion reported at downtown factory, emergency services responding\n",
      ",Classification: ON-TOPIC - EXPLOSION\n",
      ",Confidence (on-topic): 0.5165\n",
      ",Confidence (disaster type): 0.5400\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Hurricane warning in effect for coastal areas, residents advised to evacuate\n",
      ",Classification: ON-TOPIC - HURRICANE\n",
      ",Confidence (on-topic): 0.9979\n",
      ",Confidence (disaster type): 0.8700\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Just watched the latest Marvel movie and it was amazing!\n",
      ",Classification: OFF-TOPIC\n",
      ",Confidence: 0.9993\n",
      ",--------------------------------------------------------------------------------\n",
      ",Tweet: Volunteers needed urgently for flood relief efforts in affected areas\n",
      ",Classification: ON-TOPIC - FLOODS\n",
      ",Confidence (on-topic): 0.9982\n",
      ",Confidence (disaster type): 0.9000\n",
      ",--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['disaster_tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Stage 1: Use your existing BERT model for on-topic/off-topic classification\n",
    "def predict_disaster_relevance(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    encoded_text = tokenizer(text, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "    input_ids = encoded_text['input_ids'].to(device)\n",
    "    attention_mask = encoded_text['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        _, predictions = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "    return {\n",
    "        'prediction': predictions.item(),\n",
    "        'on_topic_probability': probs[0][1].item(),\n",
    "        'off_topic_probability': probs[0][0].item(),\n",
    "        'label': 'on-topic' if predictions.item() == 1 else 'off-topic'\n",
    "    }\n",
    "\n",
    "# Stage 2: Train a second classifier for disaster type identification\n",
    "# (This only needs to be trained once, then can be saved and reused)\n",
    "\n",
    "# Load dataset with disaster categories\n",
    "df = pd.read_csv(\"combined.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Filter only on-topic tweets and their categories\n",
    "on_topic_df = df[df['label'] == 'on-topic'].copy()\n",
    "\n",
    "# Ensure we have the category column\n",
    "if 'category' not in on_topic_df.columns:\n",
    "    raise ValueError(\"Your dataset needs a 'category' column for disaster types\")\n",
    "\n",
    "# Define disaster categories (modify based on your actual data)\n",
    "disaster_categories = ['bombing', 'earthquake', 'explosion', 'floods', 'hurricane', 'tornado']\n",
    "\n",
    "# Train a simple TF-IDF + RandomForest classifier for disaster type\n",
    "X = on_topic_df['tweet'].tolist()\n",
    "y = on_topic_df['category'].tolist()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train classifier\n",
    "disaster_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "disaster_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Combined prediction function\n",
    "def predict_disaster_type(text, relevance_model, disaster_model, tokenizer, tfidf_vectorizer, device):\n",
    "    # First check if the tweet is on-topic\n",
    "    relevance_result = predict_disaster_relevance(text, relevance_model, tokenizer, device)\n",
    "\n",
    "    if relevance_result['label'] == 'off-topic':\n",
    "        return {\n",
    "            'is_disaster': False,\n",
    "            'disaster_type': 'unrelated',\n",
    "            'confidence': relevance_result['off_topic_probability']\n",
    "        }\n",
    "    else:\n",
    "        # If on-topic, predict the disaster type\n",
    "        text_tfidf = tfidf_vectorizer.transform([text])\n",
    "        disaster_type = disaster_classifier.predict(text_tfidf)[0]\n",
    "        disaster_probs = disaster_classifier.predict_proba(text_tfidf)[0]\n",
    "        max_prob = max(disaster_probs)\n",
    "\n",
    "        return {\n",
    "            'is_disaster': True,\n",
    "            'disaster_type': disaster_type,\n",
    "            'confidence': relevance_result['on_topic_probability'],\n",
    "            'disaster_type_confidence': max_prob\n",
    "        }\n",
    "\n",
    "# Test with example tweets\n",
    "test_tweets = [\n",
    "    \"BREAKING NEWS: Earthquake magnitude 7.1 hits coastal region, tsunami warning issued #emergency\",\n",
    "    \"Explosion reported at downtown factory, emergency services responding\",\n",
    "    \"Hurricane warning in effect for coastal areas, residents advised to evacuate\",\n",
    "    \"Just watched the latest Marvel movie and it was amazing!\",\n",
    "    \"Volunteers needed urgently for flood relief efforts in affected areas\"\n",
    "]\n",
    "\n",
    "# Make predictions\n",
    "print(\"Two-Stage Disaster Classification Results:\\n\")\n",
    "for tweet in test_tweets:\n",
    "    result = predict_disaster_type(tweet, model, disaster_classifier, tokenizer, tfidf, device)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    if result['is_disaster']:\n",
    "        print(f\"Classification: ON-TOPIC - {result['disaster_type'].upper()}\")\n",
    "        print(f\"Confidence (on-topic): {result['confidence']:.4f}\")\n",
    "        print(f\"Confidence (disaster type): {result['disaster_type_confidence']:.4f}\")\n",
    "    else:\n",
    "        print(f\"Classification: OFF-TOPIC\")\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# If you want to save the second classifier\n",
    "import joblib\n",
    "joblib.dump(disaster_classifier, 'disaster_type_classifier.joblib')\n",
    "joblib.dump(tfidf, 'disaster_tfidf_vectorizer.joblib')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
